<h1 align="center">Hello ğŸ‘‹ I'm Neura Lumina</h1>

<p align="center">
  <em>Building small, transparent, slightly strange language models â€” mostly in pure C++</em><br>
  Exploring character-level prediction, vectorized word similarity, cycle-aware generation, and snapping AI nonsense back to real words.
</p>

<div align="center">

  ğŸŒ &nbsp; Kitwe, Zambia  
  ğŸ› ï¸ &nbsp; C++ â€¢ bigrams â€¢ histogram embeddings â€¢ cosine similarity  
  ğŸ§  &nbsp; Lightweight next-token predictors â€¢ style imitation â€¢ vocabulary-aware text  
  âš¡ &nbsp; Currently: making tiny models that still feel surprisingly alive

</div>

<br>

### ğŸ”§ What I'm usually hacking on

- Character-level bigram & trigram chains  
- Bag-of-characters histograms + cosine-based word projection  
- Greedy generation with simple cycle detection  
- Forcing generated gibberish to match a real vocabulary  
- Clean, readable C++ code (because complexity should be earned)

### ğŸ› ï¸ Flagship toy project

**[Vectmo](https://github.com/NeuraLumina/Vectmo)**  
A minimalist C++ character-level language model that:
- learns bigram statistics from raw text  
- generates continuations with basic repetition avoidance  
- projects every wild token onto your actual word list using cosine similarity on character histograms  

â†’ Produces surprisingly coherent mimicry with almost no trainable parameters

<br>

### ğŸ“« Reach out

- **X / Twitter**: [@NeuraLumina](https://twitter.com/NeuraLumina)  
- **DMs open** on X for collabs, questions, or just weird model ideas ğŸŒ

<br>

<p align="center">
  <i>"The most interesting models are the ones you can still debug at 2 a.m."</i>
</p>

---

coded by the CEO  
**Chimuka Mukwenya**
